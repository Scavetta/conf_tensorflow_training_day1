
> library(keras)

> FLAGS <- flags(flag_numeric("dropout1", 0.4), flag_numeric("dropout2", 
+     0.3))

> mnist <- dataset_mnist()

> x_train <- mnist$train$x

> y_train <- mnist$train$y

> x_test <- mnist$test$x

> y_test <- mnist$test$y

> dim(x_train) <- c(nrow(x_train), 784)

> dim(x_test) <- c(nrow(x_test), 784)

> x_train <- x_train/255

> x_test <- x_test/255

> y_train <- to_categorical(y_train, 10)

> y_test <- to_categorical(y_test, 10)

> model <- keras_model_sequential()

> model %>% layer_dense(units = 256, activation = "relu", 
+     input_shape = c(784)) %>% layer_dropout(rate = FLAGS$dropout1) %>% 
+     layer_dense .... [TRUNCATED] 

> model %>% compile(loss = "categorical_crossentropy", 
+     optimizer = optimizer_rmsprop(lr = 0.001), metrics = c("accuracy"))

> history <- model %>% fit(x_train, y_train, batch_size = 128, 
+     epochs = 20, verbose = 1, validation_split = 0.2)

> plot(history)

> score <- model %>% evaluate(x_test, y_test, verbose = 0)

> cat("Test loss:", score$loss, "\n")
Test loss: 0.09581624 

> cat("Test accuracy:", score$acc, "\n")
Test accuracy: 0.981 
